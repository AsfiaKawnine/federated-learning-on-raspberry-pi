{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FederatedLearningRaspberryPIs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikaelasanchez/federated-learning-on-raspberry-pi/blob/master/Federated%20Recurrent%20Neural%20Network/FederatedLearningRaspberryPIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9VX3Bnj0ADL",
        "colab_type": "code",
        "outputId": "8b910837-4ab9-489b-8e6c-fa73d6e8975e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Install PySyft in Google Colab\n",
        "\n",
        "!pip install tf-encrypted==0.5.6\n",
        "!pip install msgpack==0.6.1\n",
        "\n",
        "! URL=\"https://github.com/openmined/PySyft.git\" && FOLDER=\"PySyft\" && if [ ! -d $FOLDER ]; then git clone -b dev --single-branch $URL; else (cd $FOLDER && git pull $URL && cd ..); fi;\n",
        "\n",
        "!cd PySyft; python setup.py install  > /dev/null\n",
        "\n",
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('./PySyft'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "    \n",
        "!pip install --upgrade --force-reinstall lz4\n",
        "!pip install --upgrade --force-reinstall websocket\n",
        "!pip install --upgrade --force-reinstall websockets\n",
        "!pip install --upgrade --force-reinstall zstd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf-encrypted==0.5.6 in /usr/local/lib/python3.6/dist-packages (0.5.6)\n",
            "Requirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted==0.5.6) (1.14.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted==0.5.6) (1.16.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted==0.5.6) (5.1.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (0.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (1.11.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (3.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (0.1.7)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (0.33.4)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted==0.5.6) (0.15.5)\n",
            "Requirement already satisfied: msgpack==0.6.1 in /usr/local/lib/python3.6/dist-packages (0.6.1)\n",
            "From https://github.com/openmined/PySyft\n",
            " * branch              HEAD       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "Collecting lz4\n",
            "  Using cached https://files.pythonhosted.org/packages/0a/c6/96bbb3525a63ebc53ea700cc7d37ab9045542d33b4d262d0f0408ad9bbf2/lz4-2.1.10-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Installing collected packages: lz4\n",
            "  Found existing installation: lz4 2.1.10\n",
            "    Uninstalling lz4-2.1.10:\n",
            "      Successfully uninstalled lz4-2.1.10\n",
            "Successfully installed lz4-2.1.10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "lz4"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting websocket\n",
            "Collecting greenlet (from websocket)\n",
            "  Using cached https://files.pythonhosted.org/packages/bf/45/142141aa47e01a5779f0fa5a53b81f8379ce8f2b1cd13df7d2f1d751ae42/greenlet-0.4.15-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting gevent (from websocket)\n",
            "  Using cached https://files.pythonhosted.org/packages/f2/ca/5b5962361ed832847b6b2f9a2d0452c8c2f29a93baef850bb8ad067c7bf9/gevent-1.4.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Installing collected packages: greenlet, gevent, websocket\n",
            "  Found existing installation: greenlet 0.4.15\n",
            "    Uninstalling greenlet-0.4.15:\n",
            "      Successfully uninstalled greenlet-0.4.15\n",
            "  Found existing installation: gevent 1.4.0\n",
            "    Uninstalling gevent-1.4.0:\n",
            "      Successfully uninstalled gevent-1.4.0\n",
            "  Found existing installation: websocket 0.2.1\n",
            "    Uninstalling websocket-0.2.1:\n",
            "      Successfully uninstalled websocket-0.2.1\n",
            "Successfully installed gevent-1.4.0 greenlet-0.4.15 websocket-0.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "websocket"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting websockets\n",
            "  Using cached https://files.pythonhosted.org/packages/f0/4b/ad228451b1c071c5c52616b7d4298ebcfcac5ae8515ede959db19e4cd56d/websockets-8.0.2-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Installing collected packages: websockets\n",
            "  Found existing installation: websockets 8.0.2\n",
            "    Uninstalling websockets-8.0.2:\n",
            "      Successfully uninstalled websockets-8.0.2\n",
            "Successfully installed websockets-8.0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "websockets"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting zstd\n",
            "Installing collected packages: zstd\n",
            "  Found existing installation: zstd 1.4.1.0\n",
            "    Uninstalling zstd-1.4.1.0:\n",
            "      Successfully uninstalled zstd-1.4.1.0\n",
            "Successfully installed zstd-1.4.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "zstd"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZjutp4dJYTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import string\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import math\n",
        "import syft as sy\n",
        "import pandas as pd\n",
        "import random\n",
        "from syft.frameworks.torch.federated import utils\n",
        "\n",
        "from syft.workers import WebsocketClientWorker\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHWeK2dOInPy",
        "colab_type": "code",
        "outputId": "68ae38cc-fcb1-4f59-8f98-1b3f5ee1d623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-17 23:37:28--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 13.226.42.89, 13.226.42.64, 13.226.42.62, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|13.226.42.89|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip.3’\n",
            "\n",
            "data.zip.3          100%[===================>]   2.75M  6.11MB/s    in 0.5s    \n",
            "\n",
            "2019-08-17 23:37:29 (6.11 MB/s) - ‘data.zip.3’ saved [2882130/2882130]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VArqRDUbIt37",
        "colab_type": "code",
        "outputId": "6ddbca3b-aa9b-45c0-9e10-f31bfe209193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!unzip data.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "replace data/eng-fra.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFyuwgqaI_yB",
        "colab_type": "code",
        "outputId": "b7f434f9-dcfa-4cd3-9b86-48dfafb4a3f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "path = '/content/data/names/*.txt'\n",
        "\n",
        "all_letters = string.ascii_letters + \".,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "\n",
        "#Load files in the path\n",
        "def findFiles(path):\n",
        "  return glob.glob(path)\n",
        "\n",
        "#Read a file and then split to lines\n",
        "def readLines(filename):\n",
        "  lines =open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "  return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "#Convert  string to ASCII format\n",
        "def unicodeToAscii(s):\n",
        "  return ''.join(\n",
        "      c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn'\n",
        "      and c in all_letters\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "for filename in findFiles(path):\n",
        "  #print(filename)\n",
        "  category = os.path.splitext(os.path.basename(filename))[0]\n",
        "  all_categories.append(category)\n",
        "  lines = readLines(filename)\n",
        "  category_lines[category] = lines\n",
        "  \n",
        "n_categories = len(all_categories)\n",
        "\n",
        "#for names in glob.glob(path):\n",
        "  #print(names)\n",
        "  \n",
        " \n",
        "print(\"Number of categories: \" + \"\\n\" + str(n_categories))\n",
        "print(\"\\n\" + \"All categories: \")\n",
        "print(*all_categories, sep = \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of categories: \n",
            "18\n",
            "\n",
            "All categories: \n",
            "Italian\n",
            "Irish\n",
            "Dutch\n",
            "Scottish\n",
            "Vietnamese\n",
            "Portuguese\n",
            "Japanese\n",
            "French\n",
            "Chinese\n",
            "Russian\n",
            "Polish\n",
            "German\n",
            "Korean\n",
            "Greek\n",
            "English\n",
            "Arabic\n",
            "Czech\n",
            "Spanish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dfyi8UZMQnz",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA4do6B9YvLE",
        "colab_type": "code",
        "outputId": "a663d70c-73c2-47ad-c5a1-bcd056daaf74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "print(*category_lines['Polish'][:6], sep = \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adamczak\n",
            "Adamczyk\n",
            "Andrysiak\n",
            "Auttenberg\n",
            "Bartosz\n",
            "Bernard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTRGMBHov73y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LanguageDataset(Dataset):\n",
        "  # Constructor\n",
        "  def __init__(self, text, labels, transform=None):\n",
        "    self.data = text\n",
        "    self.targets = labels # categories\n",
        "    #self.to_torchtensor()\n",
        "    self.transform = transform\n",
        "    \n",
        "  def to_torchtensor(self):\n",
        "    self.data = torch.from_numpy(self.text, requires_grad=True)\n",
        "    self.labels = torch.from_numpy(self.targets, requires_grad=True)\n",
        "  \n",
        "  # Returns length of dataset/batches\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  # Returns data and target[torch tensor ]\n",
        "  def __getitem__(self, idx):\n",
        "    sample = self.data[idx]\n",
        "    target = self.targets[idx]\n",
        "    \n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "      \n",
        "    return sample, target\n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia7UiNvbAlMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Arguments for the program\n",
        "class Arguments():\n",
        "  def __init__(self):\n",
        "    self.batch_size = 1\n",
        "    self.learning_rate = 0.005\n",
        "    self.epochs = 10000\n",
        "    self.federate_after_n_batches =15000\n",
        "    self.seed = 1\n",
        "    self.print_every = 200\n",
        "    self.plot_every = 100\n",
        "    self.use_cuda = False\n",
        "    \n",
        "args = Arguments()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaHCIZtGDvEO",
        "colab_type": "code",
        "outputId": "55f4d6d4-47b0-44a0-b81f-9d506c5324ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%latex\n",
        "\n",
        "\\begin{split}\n",
        "names\\_list = [d_1,...d_n]  \\\\\n",
        "\n",
        "category\\_list = [c_1,...c_n]\n",
        "\\end{split}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/latex": "\n\\begin{split}\nnames\\_list = [d_1,...d_n]  \\\\\n\ncategory\\_list = [c_1,...c_n]\n\\end{split}",
            "text/plain": [
              "<IPython.core.display.Latex object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qtr-6GH2GRMR",
        "colab_type": "code",
        "outputId": "56eb4316-89b5-42c2-af82-9a182f4b5289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "names_list = []\n",
        "category_list = []\n",
        "\n",
        "for nation, names in category_lines.items():\n",
        "  for name in names:\n",
        "    names_list.append(name)\n",
        "    category_list.append(nation)\n",
        "    \n",
        "print(*names_list[:5], sep = \"\\n\")\n",
        "print(*category_list[:5], sep = \"\\n\")\n",
        "print(\"\\n\")\n",
        "print(\"Data points loaded: \" + str(len(names_list)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Abandonato\n",
            "Abatangelo\n",
            "Abatantuono\n",
            "Abate\n",
            "Abategiovanni\n",
            "Italian\n",
            "Italian\n",
            "Italian\n",
            "Italian\n",
            "Italian\n",
            "\n",
            "\n",
            "Data points loaded: 20074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKJZuuFzMw6H",
        "colab_type": "code",
        "outputId": "8e65b037-d8c1-47cb-d8e8-6982d8dc8810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# An integer to every category\n",
        "categories_numerical = pd.factorize(category_list)[0]\n",
        "\n",
        "# Categories with tensor\n",
        "category_tensor = torch.tensor(np.array(categories_numerical), dtype=torch.long)\n",
        "\n",
        "categories_numpy = np.array(category_tensor)\n",
        "\n",
        "print(names_list[100:120])\n",
        "print(categories_numpy[100:120])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Allegro', 'Alo', 'Aloia', 'Aloisi', 'Altamura', 'Altimari', 'Altoviti', 'Alunni', 'Amadei', 'Amadori', 'Amalberti', 'Amantea', 'Amato', 'Amatore', 'Ambrogi', 'Ambrosi', 'Amello', 'Amerighi', 'Amoretto', 'Angioli']\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIe6sXJo7Sdz",
        "colab_type": "text"
      },
      "source": [
        "We will turn every character in each input string into a vector, with a 1 marking that particular character present. <br>\n",
        "A word will just be a vector of character vectors and our RNN will process every character vector in the word.<br>\n",
        "This technique is called word embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH3mARx7Q7y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This returns the index of a letter given\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "    \n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor    \n",
        "    \n",
        "    \n",
        "# Turn a list of strings into a list of tensors\n",
        "def list_strings_to_list_tensors(names_list):\n",
        "    lines_tensors = []\n",
        "    for index, line in enumerate(names_list):\n",
        "        lineTensor = lineToTensor(line)\n",
        "        lineNumpy = lineTensor.numpy()\n",
        "        lines_tensors.append(lineNumpy)\n",
        "        \n",
        "    return(lines_tensors)\n",
        "\n",
        "lines_tensors = list_strings_to_list_tensors(names_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN5qo2w3TwRd",
        "colab_type": "code",
        "outputId": "d84e1672-abd3-4ae9-dff1-6e9ad754e39e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "# Testing the functions work\n",
        "print(names_list[0])\n",
        "print(lines_tensors[0])\n",
        "print(lines_tensors[0].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Abandonato\n",
            "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "(10, 1, 56)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAJ6cy5dQ7wR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Identify the longest word in the dataset as all tensors need to have the same\n",
        "# shape \n",
        "\n",
        "max_line_size = max(len(x) for x in lines_tensors)\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>\n",
        "def lineToTensorFillEmpty(line, max_line_size):\n",
        "    tensor = torch.zeros(max_line_size, 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "        \n",
        "    # If there is no character, a vector with (0,0,.... ,0) will be placed\n",
        "    return tensor\n",
        "\n",
        "# Turn a list of strings into a list of tensors using previous function\n",
        "def list_strings_to_list_tensors_fill_empty(names_list):\n",
        "    lines_tensors = []\n",
        "    for index, line in enumerate(names_list):\n",
        "        lineTensor = lineToTensorFillEmpty(line, max_line_size)\n",
        "        lines_tensors.append(lineTensor)\n",
        "    return(lines_tensors)\n",
        "\n",
        "lines_tensors = list_strings_to_list_tensors_fill_empty(names_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kabCi_VU7a_n",
        "colab_type": "code",
        "outputId": "656ada07-8215-44fb-dbe2-25fd3e363790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Tensor shape check\n",
        "print(lines_tensors[0].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([19, 1, 56])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1c69hd57de_",
        "colab_type": "code",
        "outputId": "1fa54b41-0db5-4feb-f52e-3bd443737f99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create numpy array with all word embeddings\n",
        "array_lines_tensors = np.stack(lines_tensors)\n",
        "array_lines_proper_dimension = np.squeeze(array_lines_tensors, axis=2)\n",
        "\n",
        "# Check array dimension\n",
        "print(array_lines_proper_dimension.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20074, 19, 56)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVnv7Zkr7fEj",
        "colab_type": "code",
        "outputId": "e7bc8910-37cc-4cca-c6ad-e21e7f25558e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def find_start_index_per_category(category_list):\n",
        "    categories_start_index = {}\n",
        "    \n",
        "    #Initialize every category with an empty list\n",
        "    for category in all_categories:\n",
        "        categories_start_index[category] = []\n",
        "    \n",
        "    #Insert the start index of each category into the dictionary categories_start_index\n",
        "    #Example: \"Italian\" --> 203\n",
        "    #         \"Spanish\" --> 19776\n",
        "    last_category = None\n",
        "    i = 0\n",
        "    for name in names_list:\n",
        "        cur_category = category_list[i]\n",
        "        if(cur_category != last_category):\n",
        "            categories_start_index[cur_category] = i\n",
        "            last_category = cur_category\n",
        "        \n",
        "        i = i + 1\n",
        "        \n",
        "    return(categories_start_index)\n",
        "\n",
        "categories_start_index = find_start_index_per_category(category_list)\n",
        "\n",
        "print(categories_start_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Italian': 0, 'Irish': 709, 'Dutch': 941, 'Scottish': 1238, 'Vietnamese': 1338, 'Portuguese': 1411, 'Japanese': 1485, 'French': 2476, 'Chinese': 2753, 'Russian': 3021, 'Polish': 12429, 'German': 12568, 'Korean': 13292, 'Greek': 13386, 'English': 13589, 'Arabic': 17257, 'Czech': 19257, 'Spanish': 19776}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4cvPiLB7g2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def randomChoice(l):\n",
        "    rand_value = random.randint(0, len(l) - 1)\n",
        "    return l[rand_value], rand_value\n",
        "\n",
        "\n",
        "def randomTrainingIndex():\n",
        "    category, rand_cat_index = randomChoice(all_categories) #cat = category, it's not a random animal\n",
        "    #rand_line_index is a relative index for a data point within the random category rand_cat_index\n",
        "    line, rand_line_index = randomChoice(category_lines[category])\n",
        "    category_start_index = categories_start_index[category]\n",
        "    absolute_index = category_start_index + rand_line_index\n",
        "    return(absolute_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMP1NjRz7ibu",
        "colab_type": "code",
        "outputId": "afee1799-7faf-442c-f01f-4aefd8922039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "#Two hidden layers, based on simple linear layers\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "#Let's instantiate the neural network already:\n",
        "n_hidden = 128\n",
        "#Instantiate RNN\n",
        "\n",
        "device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
        "model = RNN(n_letters, n_hidden, n_categories).to(device)\n",
        "#The final softmax layer will produce a probability for each one of our 18 categories\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (i2h): Linear(in_features=184, out_features=128, bias=True)\n",
            "  (i2o): Linear(in_features=184, out_features=18, bias=True)\n",
            "  (softmax): LogSoftmax()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbnoVIjf7ksO",
        "colab_type": "code",
        "outputId": "e92d5dbf-a869-4ef0-d68c-22f9b4b12424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Specify remote workers's location\n",
        "\n",
        "hook = sy.TorchHook(torch)  # Hook PyTorch\n",
        "\n",
        "# Uncomment this with the ip of each raspberry pi worker if you're using the\n",
        "# raspberry pi and comment the block of code beneath this\n",
        "\n",
        "# kwargs_websocket_alice = {\"host\": \"ip_alice\", \"hook\": hook}\n",
        "# alice = WebsocketClientWorker(id=\"alice\", port=8777, **kwargs_websocket_alice)\n",
        "# kwargs_websocket_bob = {\"host\": \"ip_bob\", \"hook\": hook}\n",
        "# bob = WebsocketClientWorker(id=\"bob\", port=8778, **kwargs_websocket_bob)\n",
        "\n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")  \n",
        "bob = sy.VirtualWorker(hook, id=\"bob\")  \n",
        "\n",
        "workers_virtual = [alice, bob]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0817 23:37:51.728178 140317129680768 hook.py:100] Torch was already hooked... skipping hooking process\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoF12vyL7mYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# array_lines_proper_dimension = our data points(X)\n",
        "# categories_numpy = our labels (Y)\n",
        "langDataset = LanguageDataset(array_lines_proper_dimension, categories_numpy)\n",
        "\n",
        "#assign the data points and the corresponding categories to workers.\n",
        "federated_train_loader = sy.FederatedDataLoader(\n",
        "    langDataset.federate(workers_virtual),\n",
        "    batch_size=args.batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz3TOccO7p3T",
        "colab_type": "text"
      },
      "source": [
        "# Model Training\n",
        "Now the data is processed, we'll start to train our RNN!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kac977c7nzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gives the category that corresponds to maximum predicted class probability\n",
        "def categoryFromOutput(output):\n",
        "    top_n, top_i = output.topk(1)\n",
        "    category_i = top_i[0].item()\n",
        "    return all_categories[category_i], category_i\n",
        "\n",
        "# Gives the amount of time passed since \"since\"\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "# Federated averaging\n",
        "def fed_avg_every_n_iters(model_pointers, iter, federate_after_n_batches):\n",
        "        models_local = {}\n",
        "        \n",
        "        if(iter % args.federate_after_n_batches == 0):\n",
        "            for worker_name, model_pointer in model_pointers.items():\n",
        "                # Assign model to the worker\n",
        "                models_local[worker_name] = model_pointer.copy().get()\n",
        "            model_avg = utils.federated_avg(models_local)\n",
        "           \n",
        "            for worker in workers_virtual:\n",
        "                model_copied_avg = model_avg.copy()\n",
        "                model_ptr = model_copied_avg.send(worker) \n",
        "                model_pointers[worker.id] = model_ptr\n",
        "                \n",
        "        return(model_pointers)     \n",
        "\n",
        "def fw_bw_pass_model(model_pointers, line_single, category_single):\n",
        "  \n",
        "    # Get the right initialized model\n",
        "    model_ptr = model_pointers[line_single.location.id]   \n",
        "    line_reshaped = line_single.reshape(max_line_size, 1, len(all_letters))\n",
        "    line_reshaped, category_single = line_reshaped.to(device), category_single.to(device)\n",
        "    \n",
        "    # Initialize hidden layer\n",
        "    hidden_init = model_ptr.initHidden() \n",
        "    \n",
        "    # And now zero the gradient\n",
        "    model_ptr.zero_grad()\n",
        "    hidden_ptr = hidden_init.send(line_single.location)\n",
        "    amount_lines_non_zero = len(torch.nonzero(line_reshaped.copy().get()))\n",
        "    \n",
        "    # Forward passes\n",
        "    for i in range(amount_lines_non_zero): \n",
        "        output, hidden_ptr = model_ptr(line_reshaped[i], hidden_ptr) \n",
        "    criterion = nn.NLLLoss()   \n",
        "    loss = criterion(output, category_single) \n",
        "    loss.backward()\n",
        "    \n",
        "    model_got = model_ptr.get() \n",
        "    \n",
        "    # Update model's weights \n",
        "    for param in model_got.parameters():\n",
        "        param.data.add_(-args.learning_rate, param.grad.data)\n",
        "        \n",
        "        \n",
        "    # Send the model\n",
        "    model_sent = model_got.send(line_single.location.id)\n",
        "    model_pointers[line_single.location.id] = model_sent\n",
        "    \n",
        "    return(model_pointers, loss, output)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVAcC1Xl7vDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training function\n",
        "def train_RNN(n_iters, print_every, plot_every, federate_after_n_batches, list_federated_train_loader):\n",
        "    current_loss = 0\n",
        "    all_losses = []    \n",
        "    \n",
        "    model_pointers = {}\n",
        "    \n",
        "    # Send the initialized model to every single worker just before training\n",
        "    for worker in workers_virtual:\n",
        "        model_copied = model.copy()\n",
        "        model_ptr = model_copied.send(worker) \n",
        "        model_pointers[worker.id] = model_ptr\n",
        "\n",
        "    # Extract a random element from the list and perform training on it\n",
        "    for iter in range(1, n_iters + 1):        \n",
        "        random_index = randomTrainingIndex()\n",
        "        line_single, category_single = list_federated_train_loader[random_index]\n",
        "        line_name = names_list[random_index]\n",
        "        model_pointers, loss, output = fw_bw_pass_model(model_pointers, line_single, category_single)\n",
        "        \n",
        "        # Update theloss\n",
        "        loss_got = loss.get().item() \n",
        "        current_loss += loss_got\n",
        "        \n",
        "        if iter % plot_every == 0:\n",
        "            all_losses.append(current_loss / plot_every)\n",
        "            current_loss = 0\n",
        "             \n",
        "        # Print information on training\n",
        "        # The name, guessed category, correct/incorrect and actual category\n",
        "        if(iter % print_every == 0):\n",
        "            output_got = output.get()\n",
        "            guess, guess_i = categoryFromOutput(output_got)\n",
        "            category = all_categories[category_single.copy().get().item()]\n",
        "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
        "            print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, \n",
        "                                                    timeSince(start), \n",
        "                                                    loss_got, \n",
        "                                                    line_name, guess, correct))\n",
        "            \n",
        "            \n",
        "    return(all_losses, model_pointers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYOR9rb67yfJ",
        "colab_type": "text"
      },
      "source": [
        "## Start the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCV84KP07xJY",
        "colab_type": "code",
        "outputId": "a5b11e69-428d-422e-c11f-7b7f78198b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        }
      },
      "source": [
        "# Turn the data points and categories into a list\n",
        "list_federated_train_loader = list(federated_train_loader)\n",
        "\n",
        "# Start the training\n",
        "start = time.time()\n",
        "all_losses, model_pointers = train_RNN(args.epochs, args.print_every, \n",
        "                                       args.plot_every, \n",
        "                                       args.federate_after_n_batches, \n",
        "                                       list_federated_train_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200 2% (0m 4s) 2.6959 Lillis / Greek ✓\n",
            "400 4% (0m 9s) 2.8750 Salomon / Spanish ✗ (Polish)\n",
            "600 6% (0m 13s) 2.7425 Michel / Italian ✗ (French)\n",
            "800 8% (0m 18s) 2.5838 Walter / English ✓\n",
            "1000 10% (0m 22s) 2.5549 Svocak / Spanish ✗ (Czech)\n",
            "1200 12% (0m 27s) 2.3157 Gouveia / Italian ✗ (Portuguese)\n",
            "1400 14% (0m 31s) 2.1592 Sergeant / Dutch ✗ (French)\n",
            "1600 16% (0m 36s) 2.5582 Alvarez / Polish ✗ (Spanish)\n",
            "1800 18% (0m 40s) 2.5616 Han / Scottish ✗ (Chinese)\n",
            "2000 20% (0m 45s) 2.2302 Mingo / English ✗ (Spanish)\n",
            "2200 22% (0m 50s) 1.2334 Hagias / Greek ✓\n",
            "2400 24% (0m 54s) 2.7290 Salazar / German ✗ (Spanish)\n",
            "2600 26% (0m 59s) 2.1957 Voltolini / French ✗ (Italian)\n",
            "2800 28% (1m 4s) 2.6820 Awdeeff / Irish ✗ (Russian)\n",
            "3000 30% (1m 8s) 2.3098 Sioda / Portuguese ✗ (Irish)\n",
            "3200 32% (1m 13s) 1.8625 Banos / Greek ✓\n",
            "3400 34% (1m 17s) 2.4356 Sinclair / Russian ✗ (Scottish)\n",
            "3600 36% (1m 22s) 1.7367 Ferguson / Scottish ✓\n",
            "3800 38% (1m 27s) 0.6339 Sklavenitis / Greek ✓\n",
            "4000 40% (1m 31s) 1.8415 Sook / Arabic ✗ (Korean)\n",
            "4200 42% (1m 36s) 1.9500 Ta / Chinese ✗ (Vietnamese)\n",
            "4400 44% (1m 40s) 2.1630 Gwock / Scottish ✗ (Chinese)\n",
            "4600 46% (1m 45s) 2.3626 Kolman / Arabic ✗ (Czech)\n",
            "4800 48% (1m 49s) 2.4149 Masanobu / Irish ✗ (Japanese)\n",
            "5000 50% (1m 54s) 1.6967 Matocha / German ✗ (Czech)\n",
            "5200 52% (1m 58s) 1.6400 Shimazaki / Japanese ✓\n",
            "5400 54% (2m 3s) 1.5407 Gwozdek / Czech ✗ (Polish)\n",
            "5600 56% (2m 8s) 1.8687 Spitznogle / Greek ✗ (German)\n",
            "5800 57% (2m 12s) 0.9645 Grokhovsky / Russian ✓\n",
            "6000 60% (2m 17s) 0.8436 Sum / Chinese ✓\n",
            "6200 62% (2m 21s) 0.7313 Kuai / Chinese ✓\n",
            "6400 64% (2m 26s) 0.7607 Tao / Chinese ✓\n",
            "6600 66% (2m 30s) 0.9656 Glynatsis / Greek ✓\n",
            "6800 68% (2m 35s) 2.2740 Wedekind / English ✗ (German)\n",
            "7000 70% (2m 39s) 2.1404 Meadhra / Japanese ✗ (Irish)\n",
            "7200 72% (2m 44s) 3.6098 Nghiem / Dutch ✗ (Vietnamese)\n",
            "7400 74% (2m 48s) 1.7172 Vennen / Irish ✗ (Dutch)\n",
            "7600 76% (2m 53s) 2.4106 Matokai / Italian ✗ (Japanese)\n",
            "7800 78% (2m 57s) 1.1387 Teoh / Chinese ✓\n",
            "8000 80% (3m 2s) 1.3217 Traversini / Italian ✓\n",
            "8200 82% (3m 6s) 1.8532 Mentis / English ✗ (Greek)\n",
            "8400 84% (3m 11s) 1.3877 Zambrano / Spanish ✓\n",
            "8600 86% (3m 16s) 2.2070 Sokoloff / Czech ✗ (Polish)\n",
            "8800 88% (3m 20s) 1.2915 Tahan / Arabic ✓\n",
            "9000 90% (3m 25s) 1.6843 Prigojy / Russian ✓\n",
            "9200 92% (3m 29s) 2.3375 Bermudez / German ✗ (Spanish)\n",
            "9400 94% (3m 34s) 2.5550 Sokal / Arabic ✗ (Polish)\n",
            "9600 96% (3m 38s) 3.1639 Jez / Korean ✗ (Polish)\n",
            "9800 98% (3m 43s) 1.6975 Cablikova / Polish ✗ (Czech)\n",
            "10000 100% (3m 47s) 1.3247 Schrijnemakers / Dutch ✓\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQzOB4Vw71rc",
        "colab_type": "code",
        "outputId": "1c035fdd-8c99-47b6-97b5-0627fcb4fc0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "# Plot the loss we got during the training procedure\n",
        "plt.figure()\n",
        "plt.title(\"Loss over time training\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel('Epochs (100s)')\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f9d847c26d8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}