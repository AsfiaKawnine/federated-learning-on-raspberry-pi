{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FederatedLearningRaspberryPIs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/helenabarmer/federated-learning-on-raspberry-pi/blob/master/FederatedLearningRaspberryPIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9VX3Bnj0ADL",
        "colab_type": "code",
        "outputId": "378ec0e1-2710-4631-f5d4-6e5e63d5aeca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Install PySyft in Google Colab\n",
        "\n",
        "!pip install tf-encrypted\n",
        "\n",
        "! URL=\"https://github.com/openmined/PySyft.git\" && FOLDER=\"PySyft\" && if [ ! -d $FOLDER ]; then git clone -b dev --single-branch $URL; else (cd $FOLDER && git pull $URL && cd ..); fi;\n",
        "\n",
        "!cd PySyft; python setup.py install  > /dev/null\n",
        "\n",
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('./PySyft'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "    \n",
        "!pip install --upgrade --force-reinstall lz4\n",
        "!pip install --upgrade --force-reinstall websocket\n",
        "!pip install --upgrade --force-reinstall websockets\n",
        "!pip install --upgrade --force-reinstall zstd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-encrypted\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/ff/7dbd5fc77fcec0df1798268a6b72a2ab0150b854761bc39c77d566798f0b/tf_encrypted-0.5.7-py3-none-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted) (1.16.4)\n",
            "Requirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted) (1.14.0)\n",
            "Collecting pyyaml>=5.1 (from tf-encrypted)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/837fefac7475963d1eccf4aa684c23b95aa6c1d033a2c5965ccb11e22623/PyYAML-5.1.1.tar.gz (274kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.12.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.1.7)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.14.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.33.4)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (3.7.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted) (2.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted) (3.1.1)\n",
            "Building wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/27/a1/775c62ddea7bfa62324fd1f65847ed31c55dadb6051481ba3f\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml, tf-encrypted\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-5.1.1 tf-encrypted-0.5.7\n",
            "Cloning into 'PySyft'...\n",
            "remote: Enumerating objects: 29220, done.\u001b[K\n",
            "remote: Total 29220 (delta 0), reused 0 (delta 0), pack-reused 29220\u001b[K\n",
            "Receiving objects: 100% (29220/29220), 32.18 MiB | 19.79 MiB/s, done.\n",
            "Resolving deltas: 100% (19444/19444), done.\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.zstd.cpython-36: module references __file__\n",
            "Collecting lz4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/c6/96bbb3525a63ebc53ea700cc7d37ab9045542d33b4d262d0f0408ad9bbf2/lz4-2.1.10-cp36-cp36m-manylinux1_x86_64.whl (385kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 2.8MB/s \n",
            "\u001b[31mERROR: syft 0.1.21a1 has requirement msgpack>=0.6.1, but you'll have msgpack 0.5.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft 0.1.21a1 has requirement tf_encrypted!=0.5.7,>=0.5.4, but you'll have tf-encrypted 0.5.7 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lz4\n",
            "  Found existing installation: lz4 2.1.10\n",
            "    Uninstalling lz4-2.1.10:\n",
            "      Successfully uninstalled lz4-2.1.10\n",
            "Successfully installed lz4-2.1.10\n",
            "Collecting websocket\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/6d/a60d620ea575c885510c574909d2e3ed62129b121fa2df00ca1c81024c87/websocket-0.2.1.tar.gz (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 2.8MB/s \n",
            "\u001b[?25hCollecting gevent (from websocket)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/ca/5b5962361ed832847b6b2f9a2d0452c8c2f29a93baef850bb8ad067c7bf9/gevent-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 18.0MB/s \n",
            "\u001b[?25hCollecting greenlet (from websocket)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/45/142141aa47e01a5779f0fa5a53b81f8379ce8f2b1cd13df7d2f1d751ae42/greenlet-0.4.15-cp36-cp36m-manylinux1_x86_64.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 16.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: websocket\n",
            "  Building wheel for websocket (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/f7/5c/9e8243838269ea93f05295708519a6e183fa6b515d9ce3b636\n",
            "Successfully built websocket\n",
            "Installing collected packages: greenlet, gevent, websocket\n",
            "  Found existing installation: greenlet 0.4.15\n",
            "    Uninstalling greenlet-0.4.15:\n",
            "      Successfully uninstalled greenlet-0.4.15\n",
            "  Found existing installation: gevent 1.4.0\n",
            "    Uninstalling gevent-1.4.0:\n",
            "      Successfully uninstalled gevent-1.4.0\n",
            "Successfully installed gevent-1.4.0 greenlet-0.4.15 websocket-0.2.1\n",
            "Collecting websockets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/d2/bf72435a7d56f94b57efdeae26c76bf0d16f409fd44ff595da745c3fbefd/websockets-8.0.1-cp36-cp36m-manylinux1_x86_64.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.2MB/s \n",
            "\u001b[31mERROR: syft 0.1.21a1 has requirement msgpack>=0.6.1, but you'll have msgpack 0.5.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft 0.1.21a1 has requirement tf_encrypted!=0.5.7,>=0.5.4, but you'll have tf-encrypted 0.5.7 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets\n",
            "  Found existing installation: websockets 8.0.1\n",
            "    Uninstalling websockets-8.0.1:\n",
            "      Successfully uninstalled websockets-8.0.1\n",
            "Successfully installed websockets-8.0.1\n",
            "Collecting zstd\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/37/6a7ba746ebddbd6cd06de84367515d6bc239acd94fb3e0b1c85788176ca2/zstd-1.4.1.0.tar.gz (454kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 2.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: zstd\n",
            "  Building wheel for zstd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/3f/ee/ac08c81af7c1b24a80c746df669ea3cb37542d27877d66ccf4\n",
            "Successfully built zstd\n",
            "\u001b[31mERROR: syft 0.1.21a1 has requirement msgpack>=0.6.1, but you'll have msgpack 0.5.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft 0.1.21a1 has requirement tf_encrypted!=0.5.7,>=0.5.4, but you'll have tf-encrypted 0.5.7 which is incompatible.\u001b[0m\n",
            "Installing collected packages: zstd\n",
            "  Found existing installation: zstd 1.4.1.0\n",
            "    Uninstalling zstd-1.4.1.0:\n",
            "      Successfully uninstalled zstd-1.4.1.0\n",
            "Successfully installed zstd-1.4.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZjutp4dJYTM",
        "colab_type": "code",
        "outputId": "ee980f09-8473-40f2-dcca-b5078c2991e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import string\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import math\n",
        "import syft as sy\n",
        "import pandas as pd\n",
        "import random\n",
        "from syft.frameworks.torch.federated import utils\n",
        "\n",
        "from syft.workers import WebsocketClientWorker\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0728 19:00:26.781474 139671392446336 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0728 19:00:26.806290 139671392446336 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHWeK2dOInPy",
        "colab_type": "code",
        "outputId": "846a0976-4168-45b4-e39c-6bc991c9bce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-28 19:00:30--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 13.226.42.62, 13.226.42.10, 13.226.42.64, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|13.226.42.62|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "\rdata.zip              0%[                    ]       0  --.-KB/s               \rdata.zip            100%[===================>]   2.75M  15.6MB/s    in 0.2s    \n",
            "\n",
            "2019-07-28 19:00:30 (15.6 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VArqRDUbIt37",
        "colab_type": "code",
        "outputId": "9147003f-d8bb-495c-de75-f26b4d9c5f8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "!unzip data.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFyuwgqaI_yB",
        "colab_type": "code",
        "outputId": "3ccafeea-3ae9-4193-9091-f4d91417c7b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "path = '/content/data/names/*.txt'\n",
        "\n",
        "all_letters = string.ascii_letters + \".,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "\n",
        "#Load files in the path\n",
        "def findFiles(path):\n",
        "  return glob.glob(path)\n",
        "\n",
        "#Read a file and then split to lines\n",
        "def readLines(filename):\n",
        "  lines =open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "  return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "#Convert  string to ASCII format\n",
        "def unicodeToAscii(s):\n",
        "  return ''.join(\n",
        "      c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn'\n",
        "      and c in all_letters\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "for filename in findFiles(path):\n",
        "  #print(filename)\n",
        "  category = os.path.splitext(os.path.basename(filename))[0]\n",
        "  all_categories.append(category)\n",
        "  lines = readLines(filename)\n",
        "  category_lines[category] = lines\n",
        "  \n",
        "n_categories = len(all_categories)\n",
        "\n",
        "#for names in glob.glob(path):\n",
        "  #print(names)\n",
        "  \n",
        " \n",
        "print(\"Number of categories: \" + \"\\n\" + str(n_categories))\n",
        "print(\"\\n\" + \"All categories: \")\n",
        "print(*all_categories, sep = \"\\n\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of categories: \n",
            "18\n",
            "\n",
            "All categories: \n",
            "Scottish\n",
            "Korean\n",
            "Dutch\n",
            "Spanish\n",
            "German\n",
            "Chinese\n",
            "Greek\n",
            "Czech\n",
            "Polish\n",
            "Arabic\n",
            "Irish\n",
            "Russian\n",
            "Portuguese\n",
            "Japanese\n",
            "English\n",
            "Vietnamese\n",
            "French\n",
            "Italian\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dfyi8UZMQnz",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA4do6B9YvLE",
        "colab_type": "code",
        "outputId": "73acaf6a-2e19-4e8a-dcfc-7e0b2b34cc27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "print(*category_lines['Polish'][:6], sep = \"\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adamczak\n",
            "Adamczyk\n",
            "Andrysiak\n",
            "Auttenberg\n",
            "Bartosz\n",
            "Bernard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTRGMBHov73y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LanguageDataset(Dataset):\n",
        "  # Constructor\n",
        "  def __init__(self, text, labels, transform=None):\n",
        "    self.data = text\n",
        "    self.targets = labels # categories\n",
        "    #self.to_torchtensor()\n",
        "    self.transform = transform\n",
        "    \n",
        "  def to_torchtensor(self):\n",
        "    self.data = torch.from_numpy(self.text, requires_grad=True)\n",
        "    self.labels = torch.from_numpy(self.targets, requires_grad=True)\n",
        "  \n",
        "  # Returns length of dataset/batches\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  # Returns data and target[torch tensor ]\n",
        "  def __getitem__(self, idx):\n",
        "    sample = self.data[idx]\n",
        "    target = self.targets[idx]\n",
        "    \n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "      \n",
        "      return sample, target\n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia7UiNvbAlMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Arguments for the program\n",
        "class Arguments():\n",
        "  def __init__(self):\n",
        "    self.batch_size = 1\n",
        "    self.learning_rate = 0.005\n",
        "    self.epochs = 10000\n",
        "    self.federate_after_n_batches =15000\n",
        "    self.seed = 1\n",
        "    self.print_every = 200\n",
        "    self.plot_every = 100\n",
        "    self.use_cuda = False\n",
        "    \n",
        "args = Arguments()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaHCIZtGDvEO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4eecac80-2ee7-43eb-e35d-de108312b930"
      },
      "source": [
        "%%latex\n",
        "\n",
        "\\begin{split}\n",
        "names\\_list = [d_1,...d_n]  \\\\\n",
        "\n",
        "category\\_list = [c_1,...c_n]\n",
        "\\end{split}"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/latex": "\n\\begin{split}\nnames\\_list = [d_1,...d_n]  \\\\\n\ncategory\\_list = [c_1,...c_n]\n\\end{split}",
            "text/plain": [
              "<IPython.core.display.Latex object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qtr-6GH2GRMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "b39fd765-67c1-4ce8-8597-6ddab4020557"
      },
      "source": [
        "names_list = []\n",
        "category_list = []\n",
        "\n",
        "for nation, names in category_lines.items():\n",
        "  for name in names:\n",
        "    names_list.append(name)\n",
        "    category_list.append(nation)\n",
        "    \n",
        "print(*names_list[:5], sep = \"\\n\")\n",
        "print(*category_list[:5], sep = \"\\n\")\n",
        "print(\"\\n\")\n",
        "print(\"Data points loaded: \" + str(len(names_list)))\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Smith\n",
            "Brown\n",
            "Wilson\n",
            "Campbell\n",
            "Stewart\n",
            "Scottish\n",
            "Scottish\n",
            "Scottish\n",
            "Scottish\n",
            "Scottish\n",
            "\n",
            "\n",
            "Data points loaded: 20074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKJZuuFzMw6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "7e27cbed-e10b-4540-bd2b-8941ba5b8bdb"
      },
      "source": [
        "# An integer to every category\n",
        "categories_numerical = pd.factorize(category_list)[0]\n",
        "\n",
        "# Categories with tensor\n",
        "category_tensor = torch.tensor(np.array(categories_numerical), dtype=torch.long)\n",
        "\n",
        "categories_numpy = np.array(category_tensor)\n",
        "\n",
        "print(names_list[100:120])\n",
        "print(categories_numpy[100:120])\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ahn', 'Baik', 'Bang', 'Byon', 'Cha', 'Chang', 'Chi', 'Chin', 'Cho', 'Choe', 'Choi', 'Chong', 'Chou', 'Chu', 'Chun', 'Chung', 'Chweh', 'Gil', 'Gu', 'Gwang']\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}